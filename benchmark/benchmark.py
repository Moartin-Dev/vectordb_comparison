#!/usr/bin/env python3
"""
Benchmark-Skript f√ºr Vektor-Datenbank Performance-Analyse
Testet PgVector vs ChromaDB mit verschiedenen OpenAPI-Spezifikationen
"""

import httpx
import asyncio
import json
import csv
import time
import statistics
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any
from dataclasses import dataclass, asdict
import argparse


@dataclass
class BenchmarkResult:
    """Datenklasse f√ºr einzelne Benchmark-Ergebnisse"""
    timestamp: str
    api_name: str
    api_provider: str
    api_category: str
    run_number: int
    num_chunks: int
    embed_ms: float
    pg_write_ms: float
    chroma_write_ms: float
    query_text: str
    query_embed_ms: float
    pg_query_ms: float
    chroma_query_ms: float
    pg_num_results: int
    chroma_num_results: int
    db_size_pg_mb: float
    db_size_chroma_mb: float


class VectorDBBenchmark:
    """Haupt-Benchmark-Klasse"""

    def __init__(self, api_url: str = "http://localhost:8000", runs_per_spec: int = 10):
        self.api_url = api_url
        self.runs_per_spec = runs_per_spec
        self.results: List[BenchmarkResult] = []
        self.total_runs = 0  # Wird in run_all_benchmarks berechnet
        self.current_run = 0  # Globaler Run-Counter

    async def fetch_spec(self, url: str) -> str:
        """L√§dt eine OpenAPI-Spezifikation von URL"""
        async with httpx.AsyncClient(timeout=60) as client:
            response = await client.get(url)
            response.raise_for_status()
            return response.text

    async def ingest_spec(self, source: str, text: str) -> Dict[str, Any]:
        """Speichert eine API-Spec in beide Datenbanken"""
        async with httpx.AsyncClient(timeout=300) as client:
            response = await client.post(
                f"{self.api_url}/ingest",
                json={
                    "source": source,
                    "text": text,
                    "backend": "both"
                }
            )
            response.raise_for_status()
            return response.json()

    async def query_spec(self, query_text: str, k: int = 5) -> Dict[str, Any]:
        """F√ºhrt eine Similarity Search durch"""
        async with httpx.AsyncClient(timeout=120) as client:
            response = await client.post(
                f"{self.api_url}/query",
                json={
                    "text": query_text,
                    "k": k
                }
            )
            response.raise_for_status()
            return response.json()

    async def get_db_stats(self) -> Dict[str, Any]:
        """Holt Datenbank-Statistiken (Gr√∂√üe, Anzahl Dokumente)"""
        async with httpx.AsyncClient(timeout=30) as client:
            response = await client.get(f"{self.api_url}/stats")
            response.raise_for_status()
            return response.json()

    async def reset_databases(self):
        """Setzt beide Datenbanken zur√ºck"""
        async with httpx.AsyncClient(timeout=60) as client:
            response = await client.post(f"{self.api_url}/reset")
            response.raise_for_status()
            return response.json()

    def generate_queries(self, api_name: str, category: str) -> List[str]:
        """Generiert relevante Testqueries basierend auf API-Typ"""
        base_queries = [
            f"API endpoints for {api_name}",
            f"authentication methods in {api_name}",
            f"data models and schemas",
            "rate limiting and quotas",
            "error handling and status codes"
        ]
        return base_queries

    def emit_progress(self, phase: str, message: str, sub_progress: float = 0.0):
        """
        Gibt strukturierten Progress-Marker aus f√ºr SSE-Tracking

        Format: [PROGRESS] phase|current_run|total_runs|sub_progress|message

        Args:
            phase: Phase-Bezeichner (run_start, reset, ingest, query, run_done)
            message: Lesbare Nachricht f√ºr Frontend
            sub_progress: Fortschritt innerhalb des aktuellen Runs (0.0-1.0)
        """
        print(f"[PROGRESS] {phase}|{self.current_run}|{self.total_runs}|{sub_progress:.2f}|{message}")
        # Flush stdout to ensure immediate delivery
        import sys
        sys.stdout.flush()

    async def run_benchmark_for_spec(self, spec_info: Dict[str, Any], category: str):
        """F√ºhrt vollst√§ndigen Benchmark f√ºr eine API-Spec durch"""
        api_name = spec_info["name"]
        api_provider = spec_info["provider"]
        url = spec_info["url"]

        print(f"\n{'='*80}")
        print(f"Benchmarking: {api_name} ({category})")
        print(f"Provider: {api_provider}")
        print(f"Runs: {self.runs_per_spec}")
        print(f"{'='*80}\n")

        # Spec herunterladen
        print(f"üì• Downloading spec from {url}...")
        try:
            spec_text = await self.fetch_spec(url)
            print(f"‚úÖ Downloaded {len(spec_text)} characters")
        except Exception as e:
            print(f"‚ùå Failed to download: {e}")
            return

        # Query-Liste generieren
        queries = self.generate_queries(api_name, category)

        # Mehrere Durchl√§ufe
        for run in range(1, self.runs_per_spec + 1):
            self.current_run += 1
            print(f"\nüîÑ Run {run}/{self.runs_per_spec}", flush=True)
            self.emit_progress("run_start", f"üîÑ Starting Run {self.current_run}/{self.total_runs}", 0.0)

            # Datenbanken zur√ºcksetzen f√ºr saubere Messung
            print("  üóëÔ∏è  Resetting databases...", flush=True)
            self.emit_progress("reset", f"üóëÔ∏è  Resetting databases (Run {self.current_run}/{self.total_runs})", 0.10)
            await self.reset_databases()
            await asyncio.sleep(1)  # Kurze Pause

            # Ingest
            print(f"  üì§ Ingesting {api_name}...", flush=True)
            self.emit_progress("ingest", f"üì§ Ingesting {api_name} (Run {self.current_run}/{self.total_runs})", 0.20)
            try:
                ingest_result = await self.ingest_spec(api_name, spec_text)
                print(f"     ‚úÖ Ingested {ingest_result['num_chunks']} chunks", flush=True)
                print(f"     ‚è±Ô∏è  Embed: {ingest_result['embed_ms']:.2f}ms", flush=True)
                print(f"     ‚è±Ô∏è  PG Write: {ingest_result['pg_write_ms']:.2f}ms", flush=True)
                print(f"     ‚è±Ô∏è  Chroma Write: {ingest_result['chroma_write_ms']:.2f}ms", flush=True)
                self.emit_progress("ingest_done", f"‚úÖ Ingest complete (Run {self.current_run}/{self.total_runs})", 0.40)
            except Exception as e:
                print(f"     ‚ùå Ingest failed: {e}")
                continue

            # DB Stats nach Ingest
            # - PG: Direkte Messung via SQL (pg_total_relation_size)
            # - ChromaDB: Berechnet basierend auf Anzahl Dokumente √ó Embedding-Gr√∂√üe
            try:
                stats_after = await self.get_db_stats()
                db_size_pg = stats_after.get("pg_size_mb", 0)
                db_size_chroma = stats_after.get("chroma_size_mb", 0)

                print(f"     üíæ DB Size - PG: {db_size_pg:.2f} MB, Chroma: {db_size_chroma:.2f} MB", flush=True)
            except Exception as e:
                print(f"     ‚ö†Ô∏è  Failed to get DB stats: {e}")
                db_size_pg = 0
                db_size_chroma = 0

            # Queries durchf√ºhren
            num_queries = len(queries)
            for query_idx, query_text in enumerate(queries, 1):
                # Berechne Progress: 40% f√ºr Ingest, 60% f√ºr Queries gleichm√§√üig verteilt
                query_progress = 0.40 + (query_idx / num_queries * 0.60)
                print(f"  üîç Querying: '{query_text[:50]}...'", flush=True)
                self.emit_progress("query", f"üîç Query {query_idx}/{num_queries}: {query_text[:40]}... (Run {self.current_run}/{self.total_runs})", query_progress)
                try:
                    query_result = await self.query_spec(query_text, k=5)

                    # Ergebnis speichern
                    result = BenchmarkResult(
                        timestamp=datetime.now().isoformat(),
                        api_name=api_name,
                        api_provider=api_provider,
                        api_category=category,
                        run_number=run,
                        num_chunks=ingest_result['num_chunks'],
                        embed_ms=ingest_result['embed_ms'],
                        pg_write_ms=ingest_result['pg_write_ms'],
                        chroma_write_ms=ingest_result['chroma_write_ms'],
                        query_text=query_text,
                        query_embed_ms=query_result['embed_ms'],
                        pg_query_ms=query_result['pg_ms'],
                        chroma_query_ms=query_result['chroma_ms'],
                        pg_num_results=len(query_result['pg_results']),
                        chroma_num_results=len(query_result['chroma_results']),
                        db_size_pg_mb=db_size_pg,
                        db_size_chroma_mb=db_size_chroma
                    )
                    self.results.append(result)

                    print(f"     ‚è±Ô∏è  PG Query: {query_result['pg_ms']:.2f}ms ({len(query_result['pg_results'])} results)")
                    print(f"     ‚è±Ô∏è  Chroma Query: {query_result['chroma_ms']:.2f}ms ({len(query_result['chroma_results'])} results)")

                except Exception as e:
                    print(f"     ‚ùå Query failed: {e}")
                    continue

            # Run abgeschlossen
            self.emit_progress("run_done", f"‚úÖ Run {self.current_run}/{self.total_runs} complete", 1.0)

            # Kurze Pause zwischen Runs
            if run < self.runs_per_spec:
                await asyncio.sleep(2)

    async def run_all_benchmarks(self, specs_file: Path, categories: List[str] = None):
        """F√ºhrt Benchmarks f√ºr alle Specs in der Liste durch"""
        print("üöÄ Starting Vector Database Benchmark Suite")
        print(f"üìä API URL: {self.api_url}")
        print(f"üîÅ Runs per spec: {self.runs_per_spec}")

        # Specs laden
        with open(specs_file, 'r') as f:
            specs_data = json.load(f)

        # Kategorien filtern
        if categories is None:
            categories = list(specs_data['categories'].keys())

        # Berechne total_runs f√ºr Progress-Tracking
        total_apis = 0
        for category in categories:
            if category in specs_data['categories']:
                total_apis += len(specs_data['categories'][category]['specs'])
        self.total_runs = self.runs_per_spec * total_apis
        print(f"üìä Total runs planned: {self.runs_per_spec} runs/API √ó {total_apis} APIs = {self.total_runs} runs")

        # F√ºr jede Kategorie
        for category in categories:
            if category not in specs_data['categories']:
                print(f"‚ö†Ô∏è  Category '{category}' not found, skipping")
                continue

            category_specs = specs_data['categories'][category]['specs']
            print(f"\nüìÅ Category: {category.upper()}")
            print(f"   {specs_data['categories'][category]['description']}")
            print(f"   APIs: {len(category_specs)}")

            for spec_info in category_specs:
                await self.run_benchmark_for_spec(spec_info, category)

        print(f"\n‚úÖ Benchmark complete! Total results: {len(self.results)}")

    def save_results(self, output_file: Path):
        """Speichert Ergebnisse als CSV"""
        if not self.results:
            print("‚ö†Ô∏è  No results to save")
            return

        with open(output_file, 'w', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=asdict(self.results[0]).keys())
            writer.writeheader()
            for result in self.results:
                writer.writerow(asdict(result))

        print(f"üíæ Results saved to: {output_file}")

    def print_summary(self):
        """Druckt Zusammenfassung der Ergebnisse"""
        if not self.results:
            print("‚ö†Ô∏è  No results to summarize")
            return

        print("\n" + "="*80)
        print("üìä BENCHMARK SUMMARY")
        print("="*80)

        # Gruppierung nach API
        apis = {}
        for result in self.results:
            if result.api_name not in apis:
                apis[result.api_name] = []
            apis[result.api_name].append(result)

        for api_name, api_results in apis.items():
            print(f"\n{api_name}:")

            # Ingest-Zeiten
            pg_writes = [r.pg_write_ms for r in api_results]
            chroma_writes = [r.chroma_write_ms for r in api_results]

            print(f"  Ingest (avg over {len(api_results)} runs):")
            print(f"    PgVector:  {statistics.mean(pg_writes):7.2f}ms (¬±{statistics.stdev(pg_writes):6.2f})")
            print(f"    ChromaDB:  {statistics.mean(chroma_writes):7.2f}ms (¬±{statistics.stdev(chroma_writes):6.2f})")

            # Query-Zeiten
            pg_queries = [r.pg_query_ms for r in api_results]
            chroma_queries = [r.chroma_query_ms for r in api_results]

            print(f"  Query (avg over {len(api_results)} queries):")
            print(f"    PgVector:  {statistics.mean(pg_queries):7.2f}ms (¬±{statistics.stdev(pg_queries):6.2f})")
            print(f"    ChromaDB:  {statistics.mean(chroma_queries):7.2f}ms (¬±{statistics.stdev(chroma_queries):6.2f})")

            # DB Gr√∂√üen
            if api_results[0].db_size_pg_mb > 0:
                print(f"  Database Size:")
                print(f"    PgVector:  {api_results[0].db_size_pg_mb:.2f} MB")
                print(f"    ChromaDB:  {api_results[0].db_size_chroma_mb:.2f} MB")


async def main():
    parser = argparse.ArgumentParser(description="Vector Database Benchmark Suite")
    parser.add_argument("--api-url", default="http://localhost:8000", help="API base URL")
    parser.add_argument("--runs", type=int, default=10, help="Number of runs per spec")
    parser.add_argument("--specs-file", default="api_specs_list.json", help="Path to specs list JSON")
    parser.add_argument("--output", default="benchmark_results.csv", help="Output CSV file")
    parser.add_argument("--categories", nargs="+", help="Categories to test (default: all)")

    args = parser.parse_args()

    # Pfade relativ zum Skript-Verzeichnis
    script_dir = Path(__file__).parent
    specs_file = script_dir / args.specs_file
    output_file = script_dir / args.output

    # Benchmark erstellen und ausf√ºhren
    benchmark = VectorDBBenchmark(api_url=args.api_url, runs_per_spec=args.runs)

    try:
        await benchmark.run_all_benchmarks(specs_file, args.categories)
        benchmark.save_results(output_file)
        benchmark.print_summary()
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è  Benchmark interrupted by user")
        if benchmark.results:
            benchmark.save_results(output_file)
            benchmark.print_summary()
    except Exception as e:
        print(f"‚ùå Benchmark failed: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())
