# ðŸš€ Benchmark Quickstart Guide

Schnelleinstieg fÃ¼r die Performance-Analyse im Rahmen der wissenschaftlichen Arbeit.

## âœ… Voraussetzungen

1. **Docker Services laufen:**
   ```bash
   cd /home/martin/Dokumente/Dev/vectordb_comparison
   docker compose ps
   ```
   Alle Services sollten "Up" sein.

2. **API ist erreichbar:**
   ```bash
   curl http://localhost:8000/health
   # Erwartete Ausgabe: {"status":"ok"}
   ```

## ðŸ“¦ Setup (Einmalig)

```bash
# In das Benchmark-Verzeichnis wechseln
cd /home/martin/Dokumente/Dev/vectordb_comparison/benchmark

# Virtual Environment erstellen
python3 -m venv venv

# Aktivieren
source venv/bin/activate

# Dependencies installieren
pip install -r requirements.txt
```

## ðŸ§ª Test-Lauf (empfohlen vor vollstÃ¤ndigem Benchmark)

```bash
# Kleiner Test mit nur 2 APIs, 3 DurchlÃ¤ufe
python benchmark.py --runs 3 --categories small

# Sollte ~2-5 Minuten dauern
# Output: benchmark_results.csv
```

**Erwartete Ausgabe:**
```
ðŸš€ Starting Vector Database Benchmark Suite
ðŸ“Š API URL: http://localhost:8000
ðŸ” Runs per spec: 3

================================================================================
Benchmarking: JSONPlaceholder (small)
...
âœ… Benchmark complete! Total results: 15
```

## ðŸ”¬ VollstÃ¤ndiger Benchmark fÃ¼r die Arbeit

```bash
# Alle APIs, 20 DurchlÃ¤ufe (empfohlen fÃ¼r statistische Signifikanz)
python benchmark.py --runs 20 --output thesis_results.csv

# Dauert ca. 30-60 Minuten, abhÃ¤ngig von:
# - Anzahl und GrÃ¶ÃŸe der APIs
# - Netzwerkgeschwindigkeit (API-Specs herunterladen)
# - Systemleistung
```

**Tipp:** Lass den Benchmark Ã¼ber Nacht laufen:
```bash
nohup python benchmark.py --runs 20 --output thesis_results.csv > benchmark.log 2>&1 &
# Fortschritt verfolgen:
tail -f benchmark.log
```

## ðŸ“Š Ergebnisse visualisieren

```bash
# Plots erstellen (300 DPI, publikationsreif)
python visualize.py thesis_results.csv --output-dir thesis_plots

# Erstellt im Verzeichnis thesis_plots/:
# - ingest_comparison.png
# - query_comparison.png
# - category_comparison.png
# - database_size_comparison.png
# - statistical_summary.png
# - statistical_summary.csv (fÃ¼r Tabellen in der Arbeit)
```

## ðŸ“ˆ Ergebnisse fÃ¼r die Arbeit verwenden

### 1. Tabellen

**Datei:** `thesis_plots/statistical_summary.csv`

Ã–ffne in Excel/LibreOffice und kopiere in deine Arbeit. EnthÃ¤lt:
- Durchschnittswerte
- Standardabweichungen
- Anzahl DurchlÃ¤ufe

### 2. Abbildungen

**Dateien:** Alle `.png` Dateien in `thesis_plots/`

- 300 DPI (druckqualitÃ¤t)
- Professionelle Seaborn-Themes
- Beschriftete Achsen
- Legende

**Beispiel-Bildunterschriften:**

> **Abbildung 1:** Vergleich der Ingest-Performance zwischen PgVector und ChromaDB fÃ¼r verschiedene API-Kategorien. Boxplot zeigt Median, Quartile und AusreiÃŸer Ã¼ber 20 DurchlÃ¤ufe.

> **Abbildung 2:** Query-Latenz-Verteilung (Violin Plot) fÃ¼r PgVector und ChromaDB. Breitere Bereiche indizieren hÃ¶here Varianz.

### 3. Statistische Auswertung

**Rohdaten:** `thesis_results.csv`

Importiere in Python/R fÃ¼r weitere Analysen:
- T-Tests
- ANOVA
- EffektstÃ¤rken
- Konfidenzintervalle

**Python-Beispiel:**
```python
import pandas as pd
from scipy import stats

df = pd.read_csv('thesis_results.csv')

# Vergleiche PgVector vs ChromaDB Ingest-Zeit
pg_ingest = df['pg_write_ms']
chroma_ingest = df['chroma_write_ms']

# T-Test
t_stat, p_value = stats.ttest_rel(pg_ingest, chroma_ingest)
print(f"T-Statistik: {t_stat:.4f}")
print(f"P-Wert: {p_value:.4f}")
```

## ðŸ”§ Anpassungen

### Eigene APIs hinzufÃ¼gen

Bearbeite `api_specs_list.json`:

```json
{
  "categories": {
    "medium": {
      "specs": [
        {
          "name": "TARDIS Internal API",
          "url": "https://your-company.com/api/openapi.yaml",
          "provider": "Your Company",
          "estimated_loc": 2000
        }
      ]
    }
  }
}
```

**Wichtig:** URL muss Ã¶ffentlich zugÃ¤nglich sein!

### Nur bestimmte APIs testen

```bash
# Nur medium und large
python benchmark.py --categories medium large --runs 10
```

### Weniger Queries pro API

Bearbeite `benchmark.py`, Zeile 86 (Funktion `generate_queries`):
```python
def generate_queries(self, api_name: str, category: str) -> List[str]:
    return [
        f"API endpoints for {api_name}",
        # Weitere Queries auskommentieren fÃ¼r schnellere Tests
    ]
```

## ðŸ› Troubleshooting

### Problem: "Connection refused"

```bash
# API-Status prÃ¼fen
docker compose logs api | tail -20

# API neu starten
cd /home/martin/Dokumente/Dev/vectordb_comparison
docker compose restart api

# Warten bis ready
sleep 10
curl http://localhost:8000/health
```

### Problem: Benchmark sehr langsam

**Ursachen:**
1. GroÃŸe API-Specs werden heruntergeladen â†’ Erste DurchlÃ¤ufe dauern lÃ¤nger
2. Ollama lÃ¤dt Embedding-Model beim ersten Mal â†’ Retry-Mechanismus greift
3. Viele Chunks â†’ Mehr Embedding-Zeit

**LÃ¶sung:**
```bash
# Nur kleine APIs testen
python benchmark.py --categories small --runs 5

# Oder weniger DurchlÃ¤ufe
python benchmark.py --runs 5
```

### Problem: "ModuleNotFoundError: No module named 'pandas'"

```bash
# Sicherstellen, dass venv aktiviert ist
source venv/bin/activate

# Requirements nochmal installieren
pip install -r requirements.txt
```

### Problem: API-Spec Download schlÃ¤gt fehl

**Beispiel-Error:**
```
âŒ Failed to download: HTTP 404
```

**LÃ¶sung:**
1. URL im Browser testen
2. In `api_specs_list.json` aktualisieren oder API entfernen
3. Alternativ: Nur funktionierende Kategorien testen:
   ```bash
   python benchmark.py --categories small
   ```

## ðŸ’¡ Best Practices

### FÃ¼r reproduzierbare Ergebnisse:

1. **Keine anderen Anwendungen laufen lassen** wÃ¤hrend Benchmark
2. **Laptop am Netzteil** (keine Energiespar-Modi)
3. **Stabile Internet-Verbindung** (fÃ¼r API-Spec Downloads)
4. **Mehrere DurchlÃ¤ufe**: Mind. 10, besser 20
5. **Dokumentieren**:
   - System-Specs (CPU, RAM, SSD)
   - Docker-Versionen (`docker --version`, `docker compose version`)
   - Datum und Uhrzeit der Benchmarks

### Beispiel-Dokumentation fÃ¼r Arbeit:

> **Testumgebung:**
> - **System:** Dell XPS 15 (Intel i7-11800H, 16GB RAM, NVMe SSD)
> - **OS:** Ubuntu 24.04 LTS
> - **Docker:** Version 24.0.7, Compose v2.23.0
> - **Datum:** 2025-01-15
> - **DurchlÃ¤ufe:** 20 pro API-Spezifikation
> - **APIs getestet:** 8 (2 small, 3 medium, 3 large)

## ðŸ“š WeiterfÃ¼hrende Informationen

- **Detaillierte Dokumentation:** `README.md`
- **API-Dokumentation:** `../CLAUDE.md`
- **Projekt-Ãœbersicht:** `../README.md`

## âœ¨ Viel Erfolg bei deiner Arbeit!

Bei Fragen oder Problemen: Dokumentation lesen oder Claude Code um Hilfe fragen ðŸ˜Š
